Zoom
Quiz
CMT
Slack
Zoom
This google doc serves as our schedule.
Barvinok 1995. Problems of distance geometry and convex properties of quadratic maps
Pataki 1998. On the rank of extreme matrices in semidefinite programs and the multiplicity of optimal eigenvalues
Barvinok 2002. A course in convexity
So, Ye, and Zhang 2008. A unified theorem on SDP rank reduction
Hazan 2008. Sparse approximate solutions to semidefinite programs
Garber and Hazan 2011. Approximating semidefinite programs in sublinear time
Jaggi 2013. Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization.
Yurtsever et al. 2017. Sketchy decisions: Convex low-rank matrix optimization with optimal storage
Burer and Monteiro 2003. A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization
Journ\'ee et al. 2010. Low-rank optimization on the cone of positive semidefinite matrices
Boumal et al. 2014. Manopt, a Matlab toolbox for optimization on manifolds
Boumal, Voroninski, and Bandeira 2016. The non-convex Burer-Monteiro approach works on smooth semidefinite programs
Bhojanapalli et al. 2018. Smoothed analysis for low-rank solutions to semidefinite programs in quadratic penalty form
Lee et al. 2016. Gradient descent only converges to minimizers
Lee et al. 2017. First-order methods almost always avoid saddle points
Gunasekar et al. 2017. Implicit Regularization in Matrix Factorization
Bhojanapalli, Neyshabur, and Srebro 2016. Global Optimality of Local Search for Low Rank Matrix Recovery
Ge, Lee, and Ma 2016. Matrix Completion has No Spurious Local Minimum
Waldspurger and Waters 2018. Rank optimality for the Burer-Monteiro factorization
Ma et al. 2017. Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion and blind deconvolution
Dauphin et al. 2014. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization
Zhang et al. 2016. Understanding deep learning requires rethinking generalization
Hao Li and 2017. Visualizing the Loss Landscape of Neural Nets
D Alberti et al. 2018. A Modern Take on the Bias-Variance Tradeoff in Neural Networks
Behnam Neyshabur et al. 2019. The role of over-parametrization in generalization of neural networks
Ian Goodfellow, Oriol Vinyals, and Andrew Saxe 2015. Qualitatively Characterizing Neural Network Optimization Problems
Hardt, Recht, and Singer 2015. Train faster, generalize better: Stability of stochastic gradient descent
Keskar et al. 2016. On large-batch training for deep learning: Generalization gap and sharp minima
Hoffer, Hubara, and Soudry 2017. Train longer, generalize better: closing the generalization gap in large batch training of neural networks
Keskar et al. 2016. On large-batch training for deep learning: Generalization gap and sharp minima
Rahimi and Recht 2009. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning
Rahimi Test of Time Talk at NeurIPS 2017
Kawaguchi, Kaelbling, and Bengio 2017. Generalization in deep learning
Neyshabur et al. 2017. Exploring Generalization in Deep Learning
Gunasekar et al. 2018. Characterizing implicit bias in terms of optimization geometry
Nacson et al. 2018. Convergence of Gradient Descent on Separable Data
1
2
3
Sinha, Namkoong, and Duchi 2017. Certifiable distributional robustness with principled adversarial training
batch normalization
Neyshabur, Salakhutdinov, and Srebro 2015. Path-sgd: Path-normalized optimization in deep neural networks
Wilson et al. 2017. The marginal value of adaptive gradient methods in machine learning
Raghu Bollapragada, Richard H. Byrd, and Jorge Nocedal 2016. Exact and Inexact Subsampled Newton Methods for Optimization
Zhewei Yao et al. 2018. Large batch size training of neural networks with adversarial training and second-order information
Chen et al. 2018. Neural Ordinary Differential Equations
Jeremy Bernstein et al. 2018. signSGD: compressed optimisation for non-convex problems
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar 2015. Generalization Bounds for Neural Networks through Tensor Factorization
Kossaifi et al. 2017. Tensor contraction layers for parsimonious deep nets
Lin et al. 2018. PacGAN: The power of two samples in generative adversarial networks
Armin Askari et al. 2018. Lifted Neural Networks
Bora et al. 2017. Compressed sensing using generative models
Chang et al. 2017. One Network to Solve Them All-Solving Linear Inverse Problems using Deep Projection Models.
Mardani et al. 2017. Deep generative adversarial networks for compressed sensing automates MRI
Mardani et al. 2018. Neural Proximal Gradient Descent for Compressive Imaging
Lucas et al. 2018. Using deep neural networks for inverse problems in imaging: beyond analytical methods
Krull, Buchholz, and Jug 2018. Noise2Void-Learning Denoising from Single Noisy Images
Recht 2018. A tour of reinforcement learning: The view from continuous control
the blog posts on this topic
Bertsekas 2018
Hardt et al. 2016. Equality of Opportunity in Supervised Learning
Hashimoto et al. 2018. Fairness Without Demographics in Repeated Loss Minimization
Goh et al. 2016. Satisfying real-world goals with dataset constraints
Blum et al. 2018. On preserving non-discrimination when combining expert advice
Liu et al. 2018. Delayed impact of fair machine learning
zoom
